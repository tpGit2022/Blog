再爬几个漫画网站练练手。本次选择漫画Claymore作为目标，进行爬取的网站有两个：[新新漫画](https://www.177mh.net/colist_83313.html)和[风之动漫](https://manhua.fzdm.com/25/)，权当加深理解了。

# ClayMore-新新漫画 

## 页面流程分析

漫画主页所在地址 https://manhua.fzdm.com/25/

从Element页面可以看出获取每一话的页面地址没有什么难度，css的选择器是 `.ar_list_col > li > a`, 会得到一系列的a标签 `<a href="/201410/290173.html" title="大剑 第155话">第155话</a>`，之后用attr和text方法提取下就行了。

![20181104155341.png](../../../../../Pictures\20181104\20181104155341.png)  

> 页面中的id:coclist1是动态生成,直接右键copy selector是不行的

之后是分析某一话中的图片链接，以 https://www.177mh.net/201312/263540.html 为例  
![20181104155812.png](../../../../../Pictures\20181104\20181104155812.png)  

  从如上的Inspect可以知道所有的图片索引就是 `<select class="selectTT">` 
的option选项，而漫画的图片地址则是 `<img id="dracga" src="https://a16d.zgxhxxmh.com:60443/h59/201312/0110245317601.jpg" style="max-width:1903px">`, 通过在Ctrl+U的源代码页页面搜索得知这两个元素都被js重新操作过，接下来需要寻找操作元素的js。

  在network标签页面，禁用缓存然后刷新页面，按住Shift移动到漫画第一页得知该图片
的请求者是`https://www.177mh.net/201410/290173.html` 和 `https://cssh.zgxhxxmh.com/tel/ct.js?v=180825`,这样就明确了后面的js操作了DOM树从而加载了漫画图片，接下来在js的代码中寻找相关代码片段。  
![20181104163443.png](../../../../../Pictures\20181104\20181104163443.png)  
  
  以漫画的img元素的id作为关键字搜索，得到相关的代码
```
var iiname = 'dracga';
function getImg(){
    var cont_img = document.getElementById(iiname);
    ....
    cont_img.src = img_qianz + arr[page - 1]
}
```

  由上可知图片链接由`img_qianz`和`arr[x]`共同组成，以这两者作为关键在js中搜索一
遍找到初始化的地方于是有了下面的代码片段：
```
$.ajax({
    url: svrss[tsvr - 1],
    cache: true,
    dataType:"script",
    type:"get",
    data: "s="+img_s+"&cid="+cid+"&coid="+coid_num,
    success: function(html) {
        img_qianz = img_qianzso[img_s];
        if (msg != "") {
            arr = msg.split("|");
            getImg();
        }

```

  从上代码段中看出img_qianz的地址可能会有变化，先考虑断点调试下，if语句处设置断
点之后F5刷新页面，之后右键选择 `evalute in console`，只有利用`console.log()`命令输出下变量值如下：  
![20181104171049.png](../../../../../Pictures\20181104\20181104171049.png)  

  由上代码段知道进行了ajax请求后进行的初始化，看到datatype是script，猜测ajax的
请求返回的JavaScript的脚本中包含了img_qianzso的声明，因为别的地方都只有对img_qianz的引用没有赋值也没有数组的声明，通过console.log输出ajax得到了各项请求参数
```
url:https://cssh.zgxhxxmh.com/img_v1/cn_svr.aspx
cache:true
datetype:"script"
type:"get"
data:s=59&cid=83313&coid=290173
```

于是fiddler构建get请求
```
https://cssh.zgxhxxmh.com/img_v1/cn_svr.aspx?s=59&cid=83313&coid=290173
```

> 根据查询到的ajax资料，为get方法时，需要将data部分拼接到链接后面

![20181104185307.png](../../../../../Pictures\20181104\20181104185307.png)  

得到结果
```
var img_qianzso = new Array();img_qianzso[59] = "https://a16d.zgxhxxmh.com:60443/h59/";
```

  果然如预期所料返回的脚本包含了img_qianzso的声明，接下来看这个ajax的请求参数，
请求的url是三个地址中选一个，为了爬取方便这里就直接用`https://cssh.zgxhxxmh.com/img_v1/cn_svr.aspx`, 之后再看主要是data参数中的img_s，cid和coid_num，在`https://cssh.zgxhxxmh.com/tel/ct.js?v=180825`中查询到:
```
var coid = /\/(\d+\/\d+)\.html/.exec(document.location);
var coid_num = /\d+\/(\d+)/.exec(coid)[1];
var cid = /\/colist_(\d+)\.html/.exec(link_z)[1];
```
  
  xxx.exec是正则匹配，看代码coid_num应该是地址中数字，别的没办法分析只能看考虑
找到相关的引用变量。经过搜索`img_s,link_z`等字眼，在html页面中发现了一段`eval(function(p,a,c,k,e,d){...}`,根据上次哥布林杀手的爬虫经验，猜测包含了相关变量的声明，于是在线js解密一波，得到结果：
```
var atsvr = "gn";
var msg = '20141...03/1104261620.jpg|201410/03/1104271623.jpg';
...
var img_s = 59;
.....
var linkname = '第155话';
var link_z = 'http://www.77mh.com/colist_83313.html';
var linkn_z = '大剑';
```
  
   如上所示得到了请求的所有参数，而且还意外的得到了msg这个参数的声明，至此图片
的`cont_img.src = img_qianz + arr[page - 1]`, 每个部分都清晰了。

   还有一个问题是实际coding中发现的，漫画图片链接`https://hws.readingbox.net/
h1/200808/2008080700091269.jpg`,下载下来的图片无法打开提示损坏。直接Chrome访问这个链接可以正常打开图片，在Chrome中右键图片另存为发现保存下来的图片依旧无法打开，F12在network面板中查看请求时在响应头中发现了如下信息：
```
HTTP/1.1 200 OK
Server: nginx
Date: Mon, 05 Nov 2018 07:58:58 GMT
Content-Type: image/webp
Content-Length: 145698
Last-Modified: Mon, 05 Nov 2018 07:13:10 GMT
Connection: keep-alive
Cache-Control: max-age=86400
ETag: "5bdfed86-23922"
Accept-Ranges: bytes
```

  上面的响应头有两项：Content-Type和ETag。看到前面的Content-Type为webp，于是猜
测图片本身是webp格式，而ETag这个响应头根据查询的资料和缓存策略有关这里就不管了.
将下载下来的图片修改后缀为webp通过[Webp在线转化](https://zhuanhuan.supfree.net/ling.asp?e=webp)转化为jpg发现打开正常，同时通过查看原来图片的十六进制数据发现: 
![20181105160542.png](../../../../../Pictures\20181105\20181105160542.png)  
   这下确定了图片链接确实指向了webp格式的图片，至于Webp转化Jpg可以看
[Google-WebpLib的使用](http://www.google.com)。

## 关键部分代码

* 爬取章节列表

```
public List<ComicEntity> getComicReelIndexUrl(String indexUrl) {
        List<ComicEntity> list = new ArrayList<>();
        try {
            Document document = Jsoup.connect(indexUrl).headers(HttpUtils.getRequestHeaders()).get();
            Elements links = document.select(".ar_list_col > li > a");
            System.out.println(links.toString());
            for (int i = 0, totalReel = links.size(); i < totalReel; i++) {
                ComicEntity  comicEntity = new ComicEntity();
                comicEntity.setReelNum(i);
                comicEntity.setReelUrl(Base_Url + links.get(i).attr("href"));
                comicEntity.setReelName(links.get(i).text());
                list.add(comicEntity);
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
        return list;
}
```

* 获取每一话中图片链接

Js脚本的执行和参数拆分

```
private String[]  execJavaScript(Element scriptElement) {
        String originDefine =  JavaScriptUtils.execJavaScriptForGoblinSlayer(scriptElement.data());
        System.out.println(originDefine);
        String[] args = originDefine.split(";");
        return args;
    }

    private String[] parseArgsAndGetImageUrl(String[] args, String pageUrl) {
        String[] imageIndexArray = args[1].substring("var msg='".length(), args[1].length() - 1).split("\\|");
        String img_s = args[3].substring("var img_s=".length(), args[3].length());
        String link_z = args[9].substring("var link_z=".length(), args[9].length());
        int startPosition =  link_z.indexOf("colist_") + "colist_".length();
        int endPosition = link_z.lastIndexOf(".html");
        String cid = link_z.substring(startPosition, endPosition);
        startPosition = pageUrl.lastIndexOf("/") + 1;
        endPosition = pageUrl.lastIndexOf(".");
        String coid_num = pageUrl.substring(startPosition, endPosition);
        System.out.println(coid_num);
        String image_prefix =  getImageUrlPre("", img_s, cid, coid_num);
        if (StringUtil.isEmptyOrNull(image_prefix)) {
            return null;
        }
        for (int i = 0; i < imageIndexArray.length; i++) {
            imageIndexArray[i]  = image_prefix + imageIndexArray[i];
        }
        return imageIndexArray;
    }

```

章节图片服务器地址获取

```
    private String getImageUrlPre(String url, String img_s, String cid, String coid_num) {
        //"s="+img_s+"&cid="+cid+"&coid="+coid_num,
        url = "https://cssh.zgxhxxmh.com/img_v1/cn_svr.aspx"; //
        String requestParams = getRequestParams(img_s, cid, coid_num);
        OkHttpClient client = new OkHttpClient();
        System.out.println(requestParams);
        Headers.Builder builder = new Headers.Builder();
        //I  try add common headers, but get unexpected result so i try add headers as less as i can
        builder.add("datatype", "script");
        builder.add("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36");
        Headers headers = builder.build();
        Request request = new Request.Builder().get().url(url + requestParams).headers(headers).build();
        Call call  = client.newCall(request);
        try {
            String response = call.execute().body().string();
            return response.substring(response.indexOf("http"), response.lastIndexOf("\""));
        } catch (IOException e) {
            e.printStackTrace();
        }
        return null;
    }

    private String getRequestParams(String img_s, String cid, String coid_num) {
        StringBuffer stringBuffer = new StringBuffer();
        stringBuffer.append("?").append("s=").append(img_s).append("&cid=").append(cid).append("&coid=").append(coid_num);
        return stringBuffer.toString();
    }
```

章节实体的填充

```
public List<ComicEntity.Chapter> getComicChapterImage(String chapterUrl) {
        List<ComicEntity.Chapter> list = new ArrayList<>();
        try {
            Document document = Jsoup.connect(chapterUrl).headers(HttpUtils.getRequestHeaders()).get();
            Element scriptElement = document.getElementsByTag("script").get(0);
            String[] args = execJavaScript(scriptElement);
            String[] imageUrlArray = parseArgsAndGetImageUrl(args, document.location());
            for (int i = 0; i < imageUrlArray.length; i++) {
                ComicEntity.Chapter chapter = new ComicEntity.Chapter();
                chapter.setChapterNum(i);
                chapter.setChapterName(String.format("%03d", i));
                chapter.setChapterImageUrl(imageUrlArray[i]);
                list.add(chapter);
            }
            return list;
        } catch (IOException e) {
            e.printStackTrace();
        }
        return null;
    }
```

章节图片的保存

```
public void saveChapterImage(String targetUrl, String reelName, String fileName) {
        Map<String, String> headers = HttpUtils.getRequestHeaders();
        OkHttpClient client = new OkHttpClient();
        Request request = new Request.Builder().get().url(targetUrl).headers(HttpUtils.getRequestHeadersForOkHttp(headers)).build();
        Call call = client.newCall(request);
        call.enqueue(new Callback() {
            @Override
            public void onFailure(Call call, IOException e) {

            }

            @Override
            public void onResponse(Call call, Response response) throws IOException {
                InputStream inputStream = response.body().byteStream();
                String contentType = response.header("Content-Type"); //image/webp
                String filePrefix = contentType.substring(contentType.indexOf("/") + 1, contentType.length());
                String newFileName =  fileName.substring(0, fileName.lastIndexOf(".") + 1) + filePrefix;
                FileUtils.saveContentToFile(inputStream, reelName, newFileName);
            }
        });
    }
```


整个流程的调用
```
public void crawlingComic(String indexUrl, String baseUrl, String baseFileDir) {
        List<ComicEntity> entities =  getComicReelIndexUrl(Index_Url);
        for (int i = 0, totalReel = entities.size(); i < totalReel; i++) {
            List<ComicEntity.Chapter> chapterList = getComicChapterImage(entities.get(i).getReelUrl());
            entities.get(i).setChapterList(chapterList);
            break;
        }

        for (int i = 0, totalReel = entities.size(); i < totalReel;i++) {
            ComicEntity entity = entities.get(i);
            List<ComicEntity.Chapter> chapterList = entity.getChapterList();
            if (chapterList == null || chapterList.size() < 1) {
                continue;
            }
            for (int j = 0, totalChapter = chapterList.size(); j < totalChapter; j++) {
                ComicEntity.Chapter chapter = chapterList.get(j);
                try {
                    Thread.sleep(5*1000);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
                System.out.println(String.format("进度 章/总章:%03d/%03d  卷/总卷:%03d/%03d url = %s", j, totalChapter, i, totalReel, chapter.getChapterImageUrl()));
                String chapterUrl = chapter.getChapterImageUrl();
                String filePrefix = chapterUrl.substring(chapterUrl.lastIndexOf("."), chapterUrl.length());
                String imageFileName = chapter.getChapterName() + filePrefix;
                String reelName = BASE_FILE_PATH + entity.getReelName() + "\\";
                saveChapterImage(chapter.getChapterImageUrl(), reelName, imageFileName);
            }
            break;
        }
    }
```


# ClayMore-风之动漫

## 爬取流程分析

  https://manhua.fzdm.com/25/ 这个网站和前面几个漫画网站不同，每一章图片都在一
个单独的URL中，页面中并没有显示章节图片总数的元素。

  经分析章节除了最后一话，页面总会包含下一页面的链接。一直没有找到类似章节图片
数组的东西，只能放弃这条思路，选择一个页面一个页面的爬取图片。

  下一章节的地址信息可以从下一页元素所在来提取，通过CSS选择器`#pjax-container 
\> div.navigation > a` 得到所有的a标签，最后一个a标签就包含有下一章节的信息，如果是最后一张，最后一个a标签包含的就是下一话的地址信息。
![20181105190235.png](../../../../../Pictures\20181105\20181105190235.png)  


  图片的真实地址在源代码页面并没有找到，考虑为js动态修改过，直接下来查询图片发
起者，通过chrome的调试工具很快确定了是html页面本身，经搜索查询到如下js代码：

```
if (/(iphone|ipod|ios|android|mobile)/i.test(navigator.userAgent.toLowerCase())) {
    $("#mhimg0").html('<a href="index_25.html"><img src="http://' + mhpicurl + '" id="mhpic" alt="大剑155话" width="100%" height="100%" onerror="nofind();" /></a>')
} else {
    $("#mhimg0").html('<a href="index_25.html"><img src="http://' + mhpicurl + '" id="mhpic" alt="大剑155话" onerror="nofind();" /></a>')
}
```

   从上述代码看出图片所在的元素是js写入的，关键在于`mhpicurl`这个变量的值，之后
又是对html页面中js代码一段搜索，得出相关的js代码如下(`---`分割不同段代码):

```
-------------------------------------------------------------------------------
var Title = "大剑155话";
var Clid = "25";
var mhurl = "2014/10/032327040.jpg";
var Url = "155";
var nexturl = "";
var CTitle = "大剑";
var mhss = getCookie("picHost");
if (mhss == "") {
    mhss = "p1.xiaoshidi.net"
}
if (mhurl.indexOf("2015") != -1 || mhurl.indexOf("2016") != -1 || mhurl.indexOf("2017") != -1 || mhurl.indexOf("2018") != -1) 
{} else {
    mhss = mhss.replace(/p1/, "p0").replace(/p2/, "p0").replace(/p07/, "p17")
}
var mhpicurl = mhss + "/" + mhurl;
if (mhurl.indexOf("http") != -1) {
    mhpicurl = mhurl
}
function nofind() {
    var e = event.srcElement ? event.srcElement: event.target;
    e.src = "http://p1.xiaoshidi.net/" + mhurl;
    var h = new Date;
    h.setTime(h.getTime() - 1);
    document.cookie = "picHost=0;path=/;domain=fzdm.com;expires=" + h.toGMTString();
    e.onerror = null;
    toastr.success("已更换为最快服务器～", {
        timeOut: 5e3
    })
};
document.write('<img src="http://' + mhpicurl + '" width="0" height="0" />'); 
}
------------------------------------------------------------------------------
if (/(iphone|ipod|ios|android|mobile)/i.test(navigator.userAgent.toLowerCase())) {
    document.writeln("<br />");
    document.writeln("<script type='text/javascript' src='https://www.yiyuantian.com/js/dongman.js'><\/script>")
} else {
    document.writeln("<script src='https://jy.ggweb.net/fz2.js'><\/script>")
}
------------------------------------------------------------------------------
if (/(iphone|ipod|ios|android|mobile)/i.test(navigator.userAgent.toLowerCase())) {
    $("#mhimg0").html('<a href="index_1.html"><img src="http://' + mhpicurl + '" id="mhpic" alt="大剑155话" width="100%" height="100%" onerror="nofind();" /></a>')
} else {
    $("#mhimg0").html('<a href="index_1.html"><img src="http://' + mhpicurl + '" id="mhpic" alt="大剑155话" onerror="nofind();" /></a>')
}
-------------------------------------------------------------------------------
var mhurl1 = "2014/10/032327041.jpg";
mhpicurl = mhss + "/" + mhurl1;
document.write('<img src="http://' + mhpicurl + '" width="0" height="0" id="mhpic1" />')
-------------------------------------------------------------------------------
```
 
  通过对上述js代码的分析得知 mhpicurl 的赋值存在三种情况：
  1. mhurl 如果http开头，那图片地址 mhpicurl 就是 mhurl
  2. mhurl 不是http开头，但是是2015-2018开头，直接用默认图片服务器地址 mhss
  3. mhurl 不是http开头，也不是2015-2018开头，需要使用新的服务器地址
  
   当然从上述js代码还可以直接得到下一页的漫画图片地址，不过考虑到本来就要下一页
下一页的这样遍历就懒得管了。

## 关键实现

* 所有章节索引爬取

```
public List<ComicEntity> getComicReelIndexUrl(String indexUrl) {
        List<ComicEntity> entities = new ArrayList<>();
        try {
            Document document = Jsoup.connect(indexUrl).headers(HttpUtils.getRequestHeaders()).get();
            Elements links = document.select("#content > li > a");
            for (int i = 0; i < links.size(); i++) {
                ComicEntity entity = new ComicEntity();
                entity.setReelNum(i);
                entity.setReelName(String.format("%03d%s", i, links.get(i).text().toString()));
                entity.setReelUrl(links.get(i).attr("href"));
                entities.add(entity);
            }
            return entities;
        } catch (IOException e) {
            e.printStackTrace();
        }
        return null;
    }
```


* 章节图片提取

漫画图片地址链接：

```
private String parseScriptElementGetImageUrl(Element element) {
        String script =  element.data();
        int startPosition = script.indexOf("var mhurl=\"") + "var mhurl=\"".length();
        int endPosition = script.indexOf("\";var Url");
        String mhur = script.substring(startPosition, endPosition);
        if (mhur.indexOf("http") != -1) {
            return mhur;
        }
        String mhss = "http://p1.xiaoshidi.net/";
        if (mhur.indexOf("2015") != -1 || mhur.indexOf("2016") != -1 || mhur.indexOf("2017") != -1 || mhur.indexOf("2018") != -1) {
            return mhss + mhur;
        }
        mhss = mhss.replace("p1", "p0").replace("p2", "p0").replace("p07", "p17");
        return  mhss + mhur;
}
```

# 总结

  原本以为经过昨天哥布林杀手漫画爬虫的实际训练，加上第一眼看去新新动漫的元素获
取都挺直观不会有折腾太久，结果还是太年轻，问题不断。还好沿着主要脉络分析下来不算太坎坷。

  第二个网站就比较狗血了，一开场就觉得不大好弄，js代码基本堆在html页面中非常难
看，分析之前的经过一系列的过滤和格式化。

# 参考资料
1. [$.ajax()方法详解](http://www.cnblogs.com/tylerdonet/p/3520862.html)
2. [Chrome Web Tool - 中文手册](http://www.css88.com/doc/chrome-devtools/network-performance/resource-loading/)